---
title: "Taller De Regresion Lineal Multiple"
author: "Andres Felipe Sabogal Ramirez"
date: "30/5/2020"
output:
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
  pdf_document: default
subtitle: Econometria I 2019-02
---
```{r global, echo=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,fig.dim = c(3, 3),fig.align="center" ,fig.subcap=TRUE,tidy=FALSE,
  split=TRUE, fig.show ="asis"
)

```

# Algunas Formulas Importantes Para Comenzar 
 $$TSS=\sum_{i=1}^{n}(y_1-\overline{y})^2$$
 
 $$ESS=\sum_{i=1}^{n}(\hat{y_i}-\overline{y})^2$$
 $$RSS=\sum_{i=1}^{n}(y_i-\hat{y_i})^2=e^2$$

# Primer Punto 
1. Se supone que la inversión es una protección contra la inflación si su precio y/o tasa
de retorno, al menos, se mantiene al ritmo de la inflación. Para probar esta hipótesis,
se decide ajustar los modelos

Modelo 1
$$P_{oro}=\beta_1+\beta_2IPC_t+\mu_t$$
Modelo 2 
$$BVNY_{t}=\beta_1+\beta_2IPC_t+\mu_t$$

## Lectura de Datos 


```{r 1.1}

rm(list=ls()) # rm remueve objetos, ls = lista
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R') #Establece el workDirectory
#install.packages("readxl") # instala el paquete para leer Excel 
library(readxl) #..library() Carga el paquete 
library(normtest)
Datos_1P <- read_excel("Datos_E1_Precio_Oro_BVNY.xls") #read_excel lee el excel
Datos_1P=data.frame(Datos_1P) # Aqui se estable que Tiempo es una hoja de Excel
attach(Datos_1P) # attach() carga los datos 
#install.packages("knitr")
library(knitr)
knitr::kable(
  head(Datos_1P), caption = "Head Datos Primer Punto",align = "c")


```

## Definimos los dos modelos de inversion

```{r 1.2}

# el modelo 1 es Poro_t= beta1 + beta2*IPC_t +U_t

Mod_1<-lm(Datos_1P$Poro~Datos_1P$IPC)
"Esta es la regresion del primer modelo"
(summary(Mod_1))
plot(Mod_1)
e1<-residuals(Mod_1)
hist(e1)
boxplot(e1, main="Boxplot residuals") # grafica de caja de los errores de estimacion (et)
"Prueba de jarque-bera"
jb.norm.test(e1) # Prueba de normalidad Jarque-Bera 
"Intervalos de Confianza"
(Intervalos_1<-confint(Mod_1))

# El modelo 2 es BVNY_t=beta1 + beta2*IPC_t + U_t

Mod_2<-lm(Datos_1P$BVNY~Datos_1P$IPC)
"Esta es la regresion del segundo modelo"
summary(Mod_2) 
plot(Mod_2)
e2<-residuals(Mod_2)
hist(e2)
boxplot(e2, main="Boxplot residuals") # grafica de caja de los errores de estimacion (et)
jb.norm.test(e2) # Prueba de normalidad Jarque-Bera 
(Intervalos_2<-confint(Mod_2))

```

Interpretacion:
Se observa que enel modelo 1 tanto el intercepto Beta1, como la pendiente Beta2 son significaticas debido a p_val<alpha, sin embargo se observa que el coeficiente de determinacion R_cuadrado es muy bajo 0.1741 por lo que solo el 17,41% de la varianza del modelo esta siedo explicada, ademas de esto la prueba de normalidad de los errores Jarque -Bera arroja un p_val<alpha 0.035 por lo que se puede pensar en la no normalidad en la distribucion de los errores
Por otra partese observa que en el modelo dos Beta1 y Beta2 son significativos al 5% con un R_cuadrado alto 0.8389 que explica el 83.39% de la varianza del modelo, y ademas la prueba J-B arroja un P_val>alpha 0.0855 por lo que rechazamos Ho y inferimos normalidad en la distribucion 

Como conclusion se obsrva que el segundo modelo es mejor ya que tiene un R2 cercano a uno y en su grafica de cuantiles se observa que se ajusta a la distribucion normal, sin embargo en la grafica Residuals Vs fitted del modelo 2 se observa que puede que el modelo no sea lineal.


# SEGUNDO PUNTO


Se formula el siguiente modelo para estudiar la relación entre X e Y: Se quiere
estimar el modelo:


$$Y_{i}=\beta_1+\beta_2X_{i}+\mu_t$$

##REGRESION GASTO-INGRESOS POR HOGARES 



```{R 2.1, fig.dim = c(6, 5)}
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R')
Datos_2P<-read_excel("Datos_E2_Gastos_Ingreso_Hogares.xls")
Datos_2P=data.frame(Datos_2P)

library(knitr)
knitr::kable(
  head(Datos_2P), caption = "Head Datos Segundo Punto",align = "c")

# Se busca estudiar la relacion que existe entre X e Y
# Se quiere estimar el modelo Y_i= beta_1+beta_2*X_i + U_i  con i=1.....40

#install.packages("ggplot2")

if (!require('ggplot2')) 
{
  install.packages('ggplot2');
  library(ggplot2);
}

library(ggplot2)
ggplot(Datos_2P, aes(x=X, y=Y)) + geom_point() + ggtitle("Gráfica Diagrama de Dispersión") + xlab("Ingresos por familia") + ylab("Gastos por familia") + geom_smooth(method=lm) 



```
En la grafica de dispersion de ingresos Vs Gastos se puede observar una relacion lineal representada por la linea azul

## Estimacion Por Metodo Matricial OLS

```{r 2.2}


X_2P <- as.matrix(cbind(1,Datos_2P[,3]))
Y_2P <- as.matrix(Datos_2P[,2])
#Reg2=lm(Y_2P~Datos_3[,3:5])

XtX_2 = t(X_2P)%*%X_2P  # Este es el resultado de multiplicar X traspuesta por X 
XtX_2  # X Traspuesta por X (Xt*X)
XtX_inv_2 <- solve(XtX_2)  # LA FUNCION solve() halla la INVERSA de la matriz 
XtX_inv_2
Xty_2 <- t(X_2P)%*%Y_2P # Resultado de X traspuesta por y ... 
Xty_2
b_2 <- XtX_inv_2%*%Xty_2 #  Resultado de los estimadores b = (Xt*X)inv*Xt*y
"Estos son los coeficientes estimados OLS"
b_2 
Ye_2 <- X_2P%*%b_2 #Calcula yt estimado
"Estos son los Y estimados por el modelo "
#OJO estos son los resultados estimados de y de acuerdo a los valores encontrados de los coeficientes 
plot(Y_2P,Ye_2, main = "Grafica Y Vs Y_estimado", )  # Se grafica Y de los datos iniciales (y)  contra Y estimado (yte)
e_2 <- Y_2P - Ye_2  # et es el vector de errores de estimacion (Ydado-Yestimado) 
(summary(e_2))  # Aqui se muestra en consola un summary de los errores de estimacion (et)

hist(e_2, prob=TRUE,main="Histogram e")   #Histograma de los errores de estimacion 
curve(dnorm(x,mean = 0,sd=1),col=1,lty=2,lwd=3,add = TRUE) # Agrega una curva normal con media en cero y desviacion estandar 1 
boxplot(e_2,main="Boxplot e") # grafica de caja de los errores de estimacion (et)
"Prueba Jarque-bera sobre los errores"
jb.norm.test(e_2) # Prueba de normalidad Jarque-Bera 
g1_qq <- qqnorm(e_2,col="black") # Genera una Q-Q plot que es un metodo para comparar distrubuciones de probabilidad de una muestra aleatoria y una de comparacion  
qqline(e_2,col="red") # qqline agrega a la grafica anterior una linea de tendencia teorica de cuantiles de una distribucion normal 

# OTRA FORMA PARA YT vs YTE
# De esta manera de grafica y y se agrega la linea LINES de yte estimado 
plot(Y_2P,t="l",main="Y Vs Y estimado") #   "l" significa linea, p significa puntos
lines(Ye_2,lty=4,col=2)
legend(x=0,y=40,legend = c("observado","estimado"),col = c(1,2),lty = c(1,4) ) #"lty" significa 1=linea, 4= punto y linea 

"Estimacion con lm"
Reg2lm=lm(X_2P~Y_2P)
(summary(Reg2lm))

```

Se observa que de los estimadores OLS del modelo tenemos un intercepto de 7.3607278 y una pendiente 0.2324681, siendo la pendiente Beta 2 se interpreta como la variacion de los  gastos promedio semanales en dólares para
hogares de tres personas (Y) freste a una variacion en los ingresos por lo que el signo es positivo, es el esperado ya que frente a un aumento en el ingreso se espera un aumento en el gasto.
El test de jarque-Bera da P_val>alpha 0,467 por lo que se acepta la Ho y se afirma la normalidad de los errores, 



## Calculo de las sumas de cuadrados TSS, ESS y RSS

```{r}
TSS_2=t(Y_2P)%*%Y_2P-length(Y_2P)*mean(Y_2P)^2  # Suma de los cuadrados totales (y-ybarra)**2
#.........Tss= SUM Y**2-n*Y_barra**2
"Este es el TSS"
TSS_2
ESS_2=t(b_2)%*% XtX_2 %*%b_2-length(Y_2P)*mean(Y_2P)^2 # Suma de los cuadrados explicados (yest-ybarra)**2   .....ESS= b'*(X'X)*b-n*Y_barra**2
"Este es el ESS"
ESS_2
RSS_2=t(e_2)%*%e_2   # suma de cuadrados residuales (yi-yest)**2 ......(et_traspuesto*et)
"Este es el RSS"
RSS_2
TSS_e16_2=ESS_2+RSS_2  # las uma de cuadrados totales es igual al RSS + ESS
"Esta es la suma TSS=ESS+RSS"
TSS_e16_2

RC_2<-ESS_2[1,1]/TSS_2[1,1]  # calculo del coeficiente de determinacion R_Cuadrado
"Este es el coeficiente de determinacion R^2"
RC_2

```
Luego el coeficiente de determinacion R2 es bajo y explica el 31.72% de la varianza del modelo 

## Estimacion De La Varianza y Pruebas De Significancia Individual

```{r}
# Recordar que b~N(beta,Sigma**2(X'X)Inv) Donde b es el vector de estimadores hallados 

# Recordar que Var b_estimado=s**2*(X'X)inv

S2_2<- RSS_2[1,1]/38 # Aqui se hala la varianza S**2estimado=RSS/(n-k)=et**2/(n-k) ES UN ESCALAR
Var_b2=S2_2*XtX_inv_2 # Matriz de varianzas y covarianzas ___Los elementos de la diagonal de la matriz contienen las varianzas de las variables, mientras que los elementos que se encuentran fuera de la diagonal contienen las covarianzas entre todos los pares posibles de variables.
S2b1_2=Var_b2[1,1]
S2b2_2=Var_b2[2,2]

sb1_2=sqrt(S2b1_2)
sb2_2=sqrt(S2b2_2)

colum1<-c("Coef","b1","b2")
colum2<-c("Estimación",b_2[1,1], b_2[2,1])
colum3<-c("Varianza",S2b1_2, S2b2_2)
colum4<-c("Error_Estandar",sb1_2, sb2_2)
Tabla_Coef_Var_2 <- cbind(colum1,colum2,colum3,colum4)
kable(Tabla_Coef_Var_2,caption = "Coeficientes y Varianzas ",align = "c")

""
"Pruebas de Significancia Individual"

"|Ho: b_j=0"  
"|Ha: b_j=/= 0"

# Recordar que t_sub_b1=b1/Se(b1)=t_sub_c  Donde Se(b1) es la desviacion estandar de b1 sqrt(var(b1))

# Recordar que P_Val= 2*Prob[t_sub_n-k > abs(t_sub_b1)]

# Aqui se hacen las pruebas de significancia individual de cada estimador 
tb1_2=b_2[1,1]/sb1_2   # VALORES t 
pb1_2=2*(1-pt(abs(tb1_2),38))  # IMPORTANTE con esta funcion pt() obtenemos la probabilidad de una distribucion t-Student con 47 grados de libertad  VALORES p 
tb2_2=b_2[2,1]/sb2_2
pb2_2=2*(1-pt(abs(tb2_2),38))

colum1<-c("Coef","b1","b2")
colum2<-c("Estimación",b_2[1,1], b_2[2,1])
colum4<-c("Error_Estandar",sb1_2, sb2_2)
colum5<-c("Valores t",tb1_2, tb2_2)
colum6<-c("Valores p",pb1_2, pb2_2)
Tabla_analisis_reg_2 <- cbind(colum1,colum2, colum4,colum5,colum6)
kable(Tabla_analisis_reg_2,caption = "Coeficientes y Varianzas ",align = "c")
```


## Modelo De Regresion Lineal En Desviaciones De La Media

```{r}

I40 <- diag(40)  # diag() nos arroja una matriz diagonal cuadrada con unos en la diagonal
i_2 <- rep(1,40)  # Genera un vector de unos de tamaño 52
iit_2 <- i_2%*% t(i_2)  # Multiplica y da coo resultado una matriz i de unos 1 de orden 52len
#vi1 <- matrix(nrow=52, ncol=1, rep(1, 52)) #  Vector de unos 
#Miit1 <- matrix(nrow=52, ncol=52, rep(1,52*52))  # Matriz de unos   
A <- I40-(1/40)*iit_2  # Matriz 52X52 donde a la diagonal principal se le resta 1/52 y el resto de entredas seran 0-1/52 
#X21 <- X0[1:52,2:5]   # Define una matriz de 52x4
Xa_2 <- A%*%X_2P[,2]  #  Resulta una matriz de 40x1 A*X2=A*X21
ya_2 <- A%*%Y_2P[,1]   # ya es un vector de 52 X 1 resultado de A*y
XatXa_2 <- t(Xa_2)%*%Xa_2 
XatXa_2
XatXa_2_inv <- solve(XatXa_2)
XatXa_2_inv
Xatya_2 <- t(Xa_2)%*%ya_2
Xatya_2
b2_desv <- XatXa_2_inv%*%Xatya_2
"b2_desv es el vector de parametros estimados en en modelo en desviaciones"
kable(b2_desv, caption = "Parametros Estimados Mod-Desv",align = "c")      # b2 es el vector de parametros estimados 

# OBTENEMOS EL INTERCEPTO
# Ybarra-b2*X2barra-b3*X3barra-b4*X4barra-b5*X5barra
Intercepto_2=mean(Y_2P)-b2_desv[1,1]*mean(X_2P[,2])
"Intercepto estimado"  # Entre comillas --muestra el mensaje en la consola

Intercepto_2


yde_2 <- Xa_2%*%b2_desv   # Vector de Y estimado 
ed_2 <-  ya_2-yde_2      # Vector de errores de estimacion 
TSSa_2=t(ya_2)%*%ya_2      # Suma de cuadrados totales 
"Suma total de cuadrados del modelo en desviaciones"
TSSa_2
ESSa_2=t(b2_desv)%*% XatXa_2 %*%b2_desv    # SUma de cuadrados Estimados  b2t*XatXa*b2
"Suma de cuadrados explicados del modelo en desviaciones"
ESSa_2
RSSa_2=t(ed_2)%*%ed_2 # Suma de cuadrados residuales eta**2
"Suma de cuadrados residuales en mod de desviaciones"
RSSa_2

c=c("Modelo en desviacioes")
c1=c("TSS","ESS","RSS")
c2=c(TSSa_2,ESSa_2,RSSa_2)
Sum_cuad_desv_2=cbind(c,c1,c2)
kable(Sum_cuad_desv_2,caption = "Sumas de Cuadrados Mod-Desv",align = "c")
"TSS=ESS+RSS"
(TAAa_2s=ESSa_2+RSSa_2)


"Prueba de normalidad Jarque-Bera"

jb.norm.test(ed_2)


```
Se puede determinar por la prueba de jarque bera que los errores del modelo 2 estimado se distribuyen de manera normal y 



# Tercer Punto 

Se tienen datos son anuales en el periodo 1947 – 2000 sobre las siguientes variables para la economía de US: C1 = Gastos de consumo real, Yd =Ingreso Real, Ri = Riqueza, y TI = Tasa de interés


Modelo P3

$$C1_{t}=\beta_1+\beta_2Yd_{t}+\beta_3RI_{t}+\beta_4TI_{t}+\mu_t$$
## Estimacion del modelo 
Beta2=Variacion enlos gastos con respecto a variacion en el ingreso real 
Beta3=Variacion en los gastos con respecto a una variacion en la riqueza disponible
Beta4=Varacion en el gasto con respecto a una variacion en la tasa de interes
Beta1=Intercepto en y= gasto autonomo


```{r}

"Estimar el modelo C1_t=B1+B2*Yd_t+B3*RI_t+B4*TI_t+U_t"

rm(list=ls()) # rm remueve objetos, ls = lista
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R') #Establece el workDirectory
#install.packages("readxl") # instala el paquete para leer Excel 
library(readxl) #..library() Carga el paquete 
Datos_3P<-read_excel("Datos_E3_Consumo.xls") #read_excel lee el excel
Datos_3P=data.frame(Datos_3P)# Aqui se estable que Tiempo es una hoja de Excel
Datos_3P$c1<-as.numeric(Datos_3P$c1)
Datos_3P$yd<-as.numeric(Datos_3P$yd)
Datos_3P$Ri<-as.numeric(Datos_3P$Ri)
Datos_3P$ti<-as.numeric(Datos_3P$ti)
attach(Datos_3P) # attach() carga los datos
kable(head(Datos_3P),caption = "Head Datos 3P ",align = "c")

library(normtest) # carga el paquete de prueba de normalidad 

X_3P <- as.matrix(cbind(1,Datos_3P[,3:5]))
Y_3P <- as.matrix(Datos_3P[,2])
XtX_3=t(X_3P)%*%X_3P  # Este es el resultado de multiplicar X traspuesta por X 
XtX_3  # X Traspuesta por X..... (X'*X)
XtX_inv_3 <- solve(XtX_3)  # LA FUNCION solve() halla la INVERSA de la matriz 
XtX_inv_3
Xty_3 <- t(X_3P)%*%Y_3P # Resultado de X traspuesta por y ... 
Xty_3
b_3 <- XtX_inv_3%*%Xty_3 #  Resultado de los estimadores b = (Xt*X)inv*Xt*y
"Estos son los coeficientes estimados OLS"
b_3
"Estos son los Y estimados por el modelo "
Ye_3 <- X_3P%*%b_3 #Calcula yt estimado
#OJO estos son los resultados estimados de y de acuerdo a los valores encontrados de los coeficientes 


plot(Y_3P,Ye_3, main = "P3 Y Vs Y_estimado")  # Se grafica Y de los datos iniciales (y)  contra Y estimado (yte)
e_3 <- Y_3P - Ye_3  # et es el vector de errores de estimacion (Ydado-Yestimado) 
summary(e_3)  # Aqui se muestra en consola un summary de los errores de estimacion (et)

hist(e_3, prob=TRUE )   #Histograma de los errores de estimacion 
curve(dnorm(x,mean = 0,sd=9),col=1,lty=2,lwd=3,add = TRUE) # Agrega una curva normal con media en cero y desviacion estandar 3,878 
boxplot(e_3, main="Boxplot e3") # grafica de caja de los errores de estimacion (et)
jb.norm.test(e_3) # Prueba de normalidad Jarque-Bera 
g1_qq <- qqnorm(e_3,col="black") # Genera una Q-Q plot que es un metodo para comparar distrubuciones de probabilidad de una muestra aleatoria y una de comparacion  
qqline(e_3,col="red") # qqline agrega a la grafica anterior una linea de tendencia teorica de cuantiles de una distribucion normal 

# OTRA FORMA PARA YT vs YTE
# De esta manera de grafica y y se agrega la linea LINES de yte estimado 

plot(Y_3P,t="l", main = "Y observado contra Y estimado 3P") #   "l" significa linea, p significa puntos
lines(Ye_3,lty=4,col=2)
legend(x=10,y=5000,legend = c("observado","estimado"),lty = c(1,4),col = c(1,2)) #"lty" significa 1=linea, 4= punto y linea 


```
Observamos en este modelo que la prueba de jarque bera tiene un p_val<alpha, sin embargo sacamos conclusiones a partir de los graficos en donde se observa que Y vs Ye tiene fuerte relacion lineal, su boxplot tiene media en cero y ademas los errores de estimacion en la qqplot se asemejan mucho a la distribucion normal

## Estimacion de la varianza y Intervalos de confianza
```{r}

#x=seq(-3,3,length=100)
#y=seq(-3,3,length=100)
#parabola=function(x,y) x^3+y^2
#z=outer(x, y, parabola)
#persp(x,y,z,theta = 45)
#image(x,y,z)
#contour(x,y,z,add=T)


# Recordar que b~N(beta,Sigma**2(X'X)Inv) Donde b es el vector de estimadores hallados 

# Recordar que Var b_estimado=s**2*(X'X)inv
RSS_3=t(e_3)%*%e_3
S2_3<- RSS_3[1,1]/51 # Aqui se hala la varianza S**2estimado=RSS/(n-k)=et**2/(n-k) ES UN ESCALAR
Var_b_3=S2_3*XtX_inv_3 # Matriz de varianzas y covarianzas ___Los elementos de la diagonal de la matriz contienen las varianzas de las variables, mientras que los elementos que se encuentran fuera de la diagonal contienen las covarianzas entre todos los pares posibles de variables.
S2b1_3=Var_b_3[1,1]
S2b2_3=Var_b_3[2,2]
S2b3_3=Var_b_3[3,3]
S2b4_3=Var_b_3[4,4]

sb1=sqrt(S2b1_3)
sb2=sqrt(S2b2_3)
sb3=sqrt(S2b3_3)
sb4=sqrt(S2b4_3)


# Recordar que para los intervalos de confianza se tiene que 
# Intervalo= b1 +- t_(alfa/2)*S_b1  con n-2 grados de libertad
ta2=qt(0.025,51,lower.tail = FALSE)   # aqui se obtiene la funcion de quantiles de la t-Student con a/2=0.025  >>>
ta2
lib1=b_3[1,1]-ta2*sb1   # Aqui se calcula el extremo superior e inferior del intervalo
lib1
lsb1=b_3[1,1]+ta2*sb1
lsb1 
lib2=b_3[2,1]-ta2*sb2
lib2
lsb2=b_3[2,1]+ta2*sb2
lsb2
lib3=b_3[3,1]-ta2*sb3
lib3
lsb3=b_3[3,1]+ta2*sb3
lsb3
lib4=b_3[4,1]-ta2*sb4
lib4
lsb4=b_3[4,1]+ta2*sb4
lsb4

colum1<-c("Coef","b1","b2","b3","b4")
colum2<-c("Estimación",b_3[1,1], b_3[2,1], b_3[3,1], b_3[4,1])
colum3<-c("Varianza",S2b1_3, S2b2_3, S2b3_3, S2b4_3)
colum4<-c("Error_Estandar",sb1, sb2, sb3, sb4)
colum5<-c("Int Inferior",lib1,lib2,lib3,lib4)
colum6<-c("Int Superior",lsb1,lsb2,lsb3,lsb4)
kable(Tabla_Coef_Var <- cbind(colum1,colum2,colum3,colum4,colum5,colum6))

```

## PRUEBA DE RESTRICCIONES LINEALES: Ho:4B2 + 3B4 = 0  ;  Ha:4B2 + 3B4 =! 0

Para esta restriccion lineal usamos la prueba F

```{r}
# Se aplica la misma prueba F que se uso para la prueba de significancia de la regresion 

R=c(0,4,0,3)
R=t(R)
R #  Matriz de coeficientes de las restricciones 2x5
r=c(0)   # vector fila r
r

# Aqui se vuelve a construir la estadistica de prueba F
p1=R%*%b_3-r  
p1
p0=XtX_inv_3%*%t(R)
"XtX_inv*Rt"
p0
p2=solve(R%*%XtX_inv_3%*%t(R))  # Inversa 
p2 
p3=t(p1)%*%p2%*%p1     
p3
fc=(p3/1)/S2_3    # Estadistica F=[(Rb-r)'(R(X'X)^-1*R')^-1*(Rb-r)/#deRestc]/S2    S2=RSS/n-k
"F_Valor "
fc
pv=pf(fc,1,51,lower.tail = F)    # Recordar que el comando pf() nos da la probabilidad de una distribucion F 
"Probabilidad _ F"
pv

# Hallamos los estimadores restringidos br

br=b_3+XtX_inv_3%*%t(R)%*%p2%*%(r-R%*%b_3)  # Donde p2=solve(R%*%XtX_inv%*%t(R))
"El estimador OLS restringido es"
br
prueba=4*br[2,1]+3*br[4,1]
"Vefificación de la restricción 1"
prueba
ye_r=X_3P%*%br  # Vector de valores y_estimados_restringidos  Se usa la matriz X para hallar el intercepto 
et_r=Y_3P-ye_r  # Vector de errores de estimacion restringidos 
RSSr=t(et_r)%*%et_r   # RSS= e^2
"El RSSr es"
RSSr
fc1=((RSSr-RSS_3)/1)/S2_3     #  Valor de F calculado para el modelo restringido 
"El Fc de la ecuación restringida (44) es"
fc1


ye_r <- X_3P%*%br   # Vector de Y estimado 
etr <-  Y_3P-ye_r      # Vector de errores de estimacion 
TSSa=t(Y_3P)%*%Y_3P      # Suma de cuadrados totales 
"TSS con restricciones lineales"
TSSa
ESSa=t(br)%*% XtX_3 %*%br    # SUma de cuadrados Estimados  b2t*XatXa*b2
"ESS con restricciones lineales"
ESSa
RSSa=t(et_r)%*%et_r # Suma de cuadrados residuales eta**2
"RSS con restricciones lineales"
RSSa

```



# Cuarto Punto

- CAR: Stock de vehículos en USA
- QMG: Consumo de gasolina en miles de galones
- PMG: Precio de la gasolina en dólares
- POP: población en miles
- RGNP: PIB real en miles de millones de dólares de 1982
- PGNP: deflactor del PIB

$$Estimacion\ del \ modelo\ 4.1\ y \ 4.2$$
$$ln(QMG)=\beta_1+\beta_2\ln(CAR)+\beta_3\ln(POT)+\beta_4\ln(RGNP)+\beta_5\ln(PGNP)+\beta_6\ln(PMG)+\mu_t\  \ (4.1)$$

$$ln(QMG/CAR)=\lambda_1+\lambda_2\ln(RGNP/POT)+\lambda_3\ln(CAR/POT)+\lambda_4\ln(PMG/PGNP)+\mu_t\  \ (4.2)$$


## Estimacion del modelo 4.1 y 4.2 con los datos 1950-1972=n1

```{r}

rm(list=ls()) # rm remueve objetos, ls = lista
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R') #Establece el workDirectory
#install.packages("readxl") # instala el paquete para leer Excel 
library(readxl) #..library() Carga el paquete 
Datos_4PT<-read_excel("Datos_E4.xls") #read_excel lee el excel
Datos_4P=data.frame(Datos_4PT[1:23,])

lnQMG=log(Datos_4P$QMG)
lnCAR=log(Datos_4P$CAR)
lnPOP=log(Datos_4P$POP)
lnRGNP=log(Datos_4P$RGNP)
lnPGNP=log(Datos_4P$PGNP)
lnPMG=log(Datos_4P$PMG)

Reg_4.1<-lm(lnQMG~lnCAR+lnPOP+lnRGNP+lnPGNP+lnPMG)
summary(Reg_4.1)

TSS_4.1=sum((lnQMG)^2)-length(lnQMG)*mean(lnQMG)^2
TSS_4.1
e_4.1=resid(Reg_4.1)
RSS_4.1=t(e_4.1)%*%e_4.1
RSS_4.11=deviance(Reg_4.1)
Ye_4.1<-fitted(Reg_4.1)
ESS_4.1=sum((Ye_4.1)^2)-length(Ye_4.1)*mean(lnQMG)^2

col1=c("ESS Modelo 4 Rest",ESS_4.1)
col2=c("TSS Modelo 4 Rest",TSS_4.1)
col3=c("RSS Modelo 4 Rest",RSS_4.1)
tab4=cbind(col1,col2,col3)
kable(tab4)

plot(Reg_4.1)

if (!require('corrplot')) 
{
  install.packages('corrplot');
  library(corrplot);
}

library("corrplot")
col4 <- colorRampPalette(c("#7F0000", "red", "#FF7F00", "yellow", "#7FFF7F",
                           "cyan", "#007FFF", "blue", "#00007F"))
MD<-cor(Datos_4P[,2:7])
corrplot(MD,method="number", title = "Grafica de Correlaciones",col = col4(20),tl.col = "black")
pairs(lnQMG~lnCAR+lnPOP+lnRGNP+lnPGNP+lnPMG,data = Datos_4P)

library(GGally)
ggpairs(Datos_4P[,2:7], lower = list(continuous = "smooth"),
        diag = list(continuous = "bar"), axisLabels = "none")

# Estimacion del modelo 4.2

lnY4=log(Datos_4P$QMG/Datos_4P$CAR)
lnx1=log(Datos_4P$RGNP/Datos_4P$POP)
lnx2=log(Datos_4P$CAR/Datos_4P$POP)
lnx3=log(Datos_4P$PMG/Datos_4P$PGNP)

Reg_4.2<-lm(lnY4~lnx1+lnx2+lnx3)
summary(Reg_4.2)

TSS_4.2=sum((lnY4)^2)-length(lnY4)*mean(lnY4)^2
e_4.2=resid(Reg_4.2)
RSS_4.2=t(e_4.2)%*%e_4.2
RSS_4.21=deviance(Reg_4.2)
Ye_4.2<-fitted(Reg_4.2)
ESS_4.2=sum((Ye_4.2)^2)-length(Ye_4.2)*mean(lnY4)^2

col1=c("ESS Modelo 4 Rest",ESS_4.2)
col2=c("TSS Modelo  Rest",TSS_4.2)
col3=c("RSS Modelo 4 Rest",RSS_4.2)
tab4.2=cbind(col1,col2,col3)
kable(tab4.2)

plot(Reg_4.2)


```
  
lnRGNP       -0.3114     0.1625  -1.916  0.07231 . 
lnPMG         1.0481     0.2682   3.907  0.00113 **
Estas son las variables significativas del modelo 3 por lo que podemos eliminarlas otras variables explicativas no significativas, podemos observar que el R2 es muy alto, explica el 99,52% de la varianza de modelo, y el p_valor < alpha, por lo tanto concluimos que la regresion es significativa.


(Intercept) -0.30653    2.37844  -0.129    0.899
lnx1        -0.13972    0.23851  -0.586    0.565
lnx2         0.05446    0.28276   0.193    0.849
lnx3         0.18527    0.27882   0.664    0.514
Observamos que en el segundo modelo no nay variables significativas, por lo que este modelo a su vez solo esplica un R2 de 0,389 lo que es un 38% de la varianza del modelo, en la grafica qqplot se observan serias desviaciones respecto de la normal teorica.



## Reestimacion de los modelos usando la muestra completa 1950-1987

```{r}
# Estimacion del modelo 4.1
"Estimacion del modelo 4.1"
lnQMGt=log(Datos_4PT$QMG)
lnCARt=log(Datos_4PT$CAR)
lnPOPt=log(Datos_4PT$POP)
lnRGNPt=log(Datos_4PT$RGNP)
lnPGNPt=log(Datos_4PT$PGNP)
lnPMGt=log(Datos_4PT$PMG)
X4.1=cbind(lnCARt,lnPOPt,lnRGNPt,lnPGNPt,lnPMGt)


Reg_4.1T<-lm(lnQMGt~lnCARt+lnPOPt+lnRGNPt+lnPGNPt+lnPMGt)
summary(Reg_4.1T)
plot(Reg_4.1T)
# Estimacion del modelo 4.2
"Estimacion del modelo 4.2"

lnY4t=log(Datos_4PT$QMG/Datos_4PT$CAR)
lnx1t=log(Datos_4PT$RGNP/Datos_4PT$POP)
lnx2t=log(Datos_4PT$CAR/Datos_4PT$POP)
lnx3t=log(Datos_4PT$PMG/Datos_4PT$PGNP)
X4=cbind(lnx1t,lnx2t,lnx3t)


Reg_4.2T<-lm(lnY4t~lnx1t+lnx2t+lnx3t)
summary(Reg_4.2T)
(RSS_R=deviance(Reg_4.1T))


"Regresion con la muestra 2 n2"
Reg_4.2F<-lm(lnY4t[23:38]~lnx1t[23:38]+lnx2t[23:38]+lnx3t[23:38])
summary(Reg_4.2F)
(RSS4F=deviance(Reg_4.2F))
```
Se observa que 
(Intercept)  9.98698    2.62062   3.811 0.000594 ***
lnCARt       2.55992    0.22813  11.221 1.26e-12 ***
lnPOPt      -2.87808    0.45344  -6.347 3.99e-07 ***
lnRGNPt     -0.42927    0.14838  -2.893 0.006811 ** 
lnPGNPt     -0.17887    0.06336  -2.823 0.008115 ** 
lnPMGt      -0.14111    0.04340  -3.252 0.002704 **
Ahora cuando tomamos toda la muestra y estimamos el modelo, ahora todos los coeficientes son significativos, por lo que se observa un mejor ajuste, tambien debido a que el R2 incremento y ahora es de 0,9927 un 99,27%de la varianza total explicada, muy buen auste, finalmente el p_val de la prueba es < alpha por lo que la regresion es significativa

No con la misma suerte observamos que el modelo 2 cuando se toman todos los datos para la estimacion los coeficientes estimados no se vuelven significativos, sin embargo tiene un buen R2 0,8274
(Intercept)   4.05308    5.68468   0.713    0.489
lnx1t[23:38] -0.29873    0.45431  -0.658    0.523
lnx2t[23:38] -0.94261    0.56801  -1.660    0.123
lnx3t[23:38] -0.09151    0.07782  -1.176    0.262


## CAMBIO ESTRUCTURAL CON VARIABLES DUMMY

pruebe si la demanda de gasolina por automóvil sufrió un cambio permanente luego del embargo petrolero de 1973.

Usamos el modelo 4.2 logaritmo de la demanda de gasolina por automobil, tener en cuenta que el modelo restringido es el modelo original con todos los datos

$$Modelo\ no\ restringido\ con\ variables\ dummy $$
$$Y_t=\beta_1+\delta_1D_t+\beta_2X_{2t}+\delta_2D_tX_{2t}+\beta_3X_{3t}+\delta_3D_3X_{3t}$$
$$ln(\frac{QMG}{CAR})=\lambda_1+\delta_1D_t+\lambda_2\ln(\frac{RGNP}{POT})+\delta_2D_t\ln(\frac{RGNP}{POT})+\lambda_3\ln(\frac{CAR}{POT})+\delta_3D_t\ln(\frac{CAR}{POT})+\lambda_4\ln(\frac{PMG}{PGNP})+\delta_4D_t\ln(\frac{PMG}{PGNP})+\mu_t$$


```{r}


#Acontinuacion procedemos a estimar el modelo 4.2 con variables Dummy
library(readxl) 
Datos_4PT<-read_excel("Datos_E4.xls") #read_excel lee el excel
d1=cbind(rep(0,23))
d2=cbind(rep(1,15))
d=rbind(d1,d2)


eq1_mnr_d=lm(lnY4t~d+X4+d*X4)
summary(eq1_mnr_d)


"Suma de cuadrados de los errores del modelo no restringido con DUMMY"
(RSS_NRD2=deviance(eq1_mnr_d))
"prueba de cambio estructural con variables Dummy"
"La estadística F de la ecuación (22) calculada"
(FC_22=((RSS_R-RSS_NRD2)/5)/(RSS_NRD2/28))
"verificación de coeficientes"
(PV_F_22=pf(FC_22,5,28,lower.tail = F))
"El valor p de la prueba nos indica que debemos rechazar H0 si p_val<<alpha; por tanto, concluimos que hay cambio estructural en la función a partir del año 1972."

bnrd=cbind(coef(eq1_mnr_d))
bnrd
b11nrd=bnrd[1]
delta1=bnrd[2]
b12nrd=b11nrd+delta1
b21nrd=bnrd[3]
delta2=bnrd[4]
b22nrd=b21nrd+delta2

```
Mediante el uso de las variables ficticias Dummy podemos concluir que hay o no un cambio estructural en el modelo a partir de cierto punto.
El valor p de la prueba nos indica que debemos rechazar H0 si p_val<<alpha; por tanto, concluimos que hay cambio estructural en la función a partir del año 1972.
Esto explica el cambio en los estimadores encontrados en cada una de las muestras


## Estimacion del modelo 4.1 por variables Dummy

```{r}
# i) Estimacion del modelo 4.1 por variables Dummy
# Construya una regresión con variable dummy para probar si la elasticidad precio cambió luego de 1973.


Reg_4.1D<-lm(lnQMGt~d+X4.1+d*X4.1)
summary(Reg_4.1D)
"RSS No Restringido Dummy Lm"
(RSS_NRDlm=deviance(Reg_4.1D))

X1=cbind(1,X4.1[1:23,])
X2=cbind(1,X4.1[24:38,])
O23=matrix(nrow=23,ncol=6,0)
O15=matrix(nrow=15,ncol=6,0)
A1=rbind(X1,O15)
A2=rbind(O23,X2)
"MATRIZ XNR"
XNR=cbind(A1,A2)

"LA MATRIZ XNRtXNR"
XNRtXNR=t(XNR)%*%XNR
"LA MATRIZ XNRtXNR inversa"
XNRtXNR_inv=solve(XNRtXNR)
"EL VECTOR XNRty"
XNRty=t(XNR)%*%lnQMGt
"EL ESTIMADOR OLS DEL MODELO NO RESTRINGIDO ES"
(bnr=XNRtXNR_inv%*%XNRty)


etnr_D=lnQMGt-XNR%*%bnr
"SUMA DE CUADRADOS DE ERRROES NR modelo Dummy Matriz"
(RSS_NRDM=t(etnr_D)%*%etnr_D)
"SUMA DE CUADRADOS EXPLICADA NR modelo Dummy Matriz "
(ESS_NRD=(t(bnr)%*%XNRtXNR%*%bnr)[1:1]-38*mean(lnQMGt)^2)
"LA SUMA TOTAL CUADRADOS NR modelo Dummy Matriz"
(TSS_NRD=(t(lnQMGt)%*%lnQMGt)[1:1]-38*mean(lnQMGt)^2)
"PRUEBA DE SUMAS DE CUADRADOS EN EL MODELO NR"
(TSS_NRD_PRUEBA=ESS_NRD+RSS_NRDM)


"ESTADÍSTICA F DE LA ECUACION (20)"
"EL VALOR CALCULADO DE LA ESTADÍSTICA ES"
(FC_20=(((RSS_R-RSS_NRDM)/5)/(RSS_NRDM/28)))
"EL P VALOR ES"
(PV_F_20=pf(FC_20,5,28,lower.tail = F))
"El valor p de la prueba nos indica que debemos rechazar H0 si p_val<<alpha; por tanto, concluimos que hay cambio estructural en la función a partir del año 1972."
```
 El valor p de la prueba nos indica que debemos rechazar H0 si p_val<<alpha; por tanto, concluimos que hay cambio estructural en la función a partir del año 1972 en el modelo 2 Verificamos esto haciendo la comparacion de los estimadores no restringidos del modelo Dummy con los estimadores restringidos, observamos que las pendientes y el intercepto son los mismos, pero en el modelo NR se le suman valores delta diferentes de cero, por lo que la hipotesis del cambio estructural se acepta

# Quinto Punto

En un paper de 1963, Marc Nerlove analizó una función de costos para 145 compañías eléctricas estadounidenses. El archivo “Datos_E3_Nerlove” contiene la información para

* TC: costo total en millones de dólares
* Q: producción en miles de millones de kilovatios hora
* PL: precio del trabajo
* PF: precio de combustibles
* PK: precio del capital

## Lectura de Datos 5P
```{r}
rm(list=ls()) # rm remueve objetos, ls = lista
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R') #Establece el workDirectory
#install.packages("readxl") # instala el paquete para leer Excel 
library(readxl) #..library() Carga el paquete 
Datos_5P<-read_excel("Datos_E5_Nerlove.xls") #read_excel lee el excel
Datos_5P=data.frame(Datos_5P)

kable(head(Datos_5P))

```

## Estimacion Modelo 5

Estime un modelo en el que se tenga como variable dependiente los costos y
como variables explicativas una constante y el resto de variables listadas. Con
esta estimación obtenga las elasticidades de los costos a las distintas variables
explicativas. Interprete los principales resultados.

```{r}
Y5=Datos_5P[,1] #  en este caso Y5 es el vector de costos 

logx2=log(Datos_5P[,2])
logx3=log(Datos_5P[,3])
logx4=log(Datos_5P[,4])
logx5=log(Datos_5P[,5])
lnY5=log(Y5)


Reg_5<-lm(lnY5~logx2+logx3+logx4+logx5)
summary(Reg_5)
(RSS_NR5=deviance(Reg_5))
plot(Reg_5)

library(normtest)
e5=resid(Reg_5)
jb.norm.test(e5)

Reg_5a<-lm(lnY5~logx2+logx4)
summary(Reg_5a)
plot(Reg_5a)

library(normtest)
e5a=resid(Reg_5a)
jb.norm.test(e5a)
```
Observamos en las graficas que los residuales se distribuyen al rededor de cero en la grafica residuals-fitted, se observan algunos valores outliers, en la grafica de cuantiles observamos que los residuales se asemejan a los cuantiles teoricos de la normal, sin embargo en la grafica de los residuales estandarizados se observa cierto comportamiento no lineal, al parecer cuadratico
Obseravamos que al eliminar los estimadores no significativos y hacer de nuevo la regresion el modelo no aumenta su R2, aunque ya es alto 98% no varia y por lo tanto procedemos a hacer el test de jarque bera el cual arroja un p-valor<alpha por lo que se rechaza ho y se concluye que no hay normalidad en los errores y por tanto en la dist prob

(Intercept) -4.53676    0.33825 -13.412  < 2e-16 ***
logx2        0.72415    0.01742  41.562  < 2e-16 ***
logx4        0.47163    0.09286   5.079 1.17e-06 ***


##Modelo 5 Restringido
Restriccion de Homogeneidad 
beta_3+beta_4+beta_5=1

```{r}
"ESTIMACIÓN DEL MODELO RESTINGIDO CON LA FUNCIÓN lm()"
#if (!require("restriktor")) install.packages("restriktor")
# Se usa el paquete "restriktor" para imponer la restricción beta3=0 sobre la ecuación estimada del modelo no restringido


if (!require("restriktor")) 
{
  install.packages('restriktor');
  library(restriktor);
}
library(restriktor)
#options(digits=3) # Para este ejemplo solo se utilizan 3 digitos decimales
Restricciones <- '
logx3+logx4+logx5 ==1;'
Reg_5R = restriktor(Reg_5, constraints = Restricciones)
summary(Reg_5R)
"Estos son los estimadores OLS del modelo 5 restringido"
(b5R=coef(Reg_5R))

sR5_mr=summary(Reg_5R)
"LA VARIANZA ESTIMADA DEL MODELO RESTRINGIDO ES"
S2_mr=sR5_mr$s2
S2_mr
" EL ERROR ESTÁNDAR DEL MODELO RESTRINGIDO ES"
s_mr=(sR5_mr$s2)^0.5
s_mr
"LA SUMA DE CUADRADOS DE LOS ERROES DEL MODELO RESTRINGIDO ES: RSS_R=S2_mr*(n-(k-1))"
RSS_R5=141*S2_mr
RSS_R5

```
(Intercept) -4.6907891  0.8848713 -5.3011 4.379e-07 ***
logx2        0.7206875  0.0174357 41.3340 < 2.2e-16 ***
logx3        0.5929096  0.2045722  2.8983  0.004357 ** 
logx4        0.4144715  0.0989512  4.1886 4.940e-05 ***

Se observa que en el modelo con la restriccion de homogeneidad el modelo se ajusta mas y todos sus estimadores a excepcion de logx5 son significativos y tiene un r2 del 92% de explicacion 

##Prueba de wald


```{r}
"ESTADÍSTICA F DE WALD DE LA ECUACIÓN F (20)"
"El valor calculado de esta estadística es:"
"FC_20"
(FC_20=((RSS_R5-RSS_NR5)/5)/(RSS_NR5/145-2*5))
"EL VALOR P PARA LA PRUEBA ES"
(pv_wald=pchisq(FC_20,df=1,lower.tail = F))

"El Test de Wald es un contraste de hipótesis donde se trata de ver la coherencia de afirmar un valor concreto de un parámetro de un modelo probabilístico una vez tenemos ya un modelo previamente seleccionado y ajustado."

"El p-valor de la F es"
(PV_F_20=pf(FC_20,5,135,lower.tail = F))
#Es este caso el anterior es es mismo estadistico de la prueba de Chow
```
## Secto Punto

El siguiente modelo puede ser utilizado para estudiar si los gastos de una campaña
afectan los resultados de la elección:

Vote A = porcentaje de votos del candidato A
expendA= gastos de campaña candidato A
expendB= gastos de campaña candidato B
prtystrA= porcentaje de votos obtenido por el pardito de A en la elección más reciente

#Estimacion del Modelo 

```{r}

rm(list=ls()) # rm remueve objetos, ls = lista
setwd('C://Users/f-pis/Desktop/Semestres/Archivos R') #Establece el workDirectory
#install.packages("readxl") # instala el paquete para leer Excel 
library(readxl) #..library() Carga el paquete 
Datos_6P<-read_excel("Datos_E6_Votacion.xls") #read_excel lee el excel
Datos_6P=data.frame(Datos_6P)
kable(head(Datos_6P))

voteA=Datos_6P[,1]
logExpendA=log(Datos_6P[,2])
logExpendB=log(Datos_6P[,3])
prtystrA=Datos_6P[,4]

Reg_6=lm(voteA~logExpendA+logExpendB+prtystrA)
summary(Reg_6)
RSS_NR6=deviance(Reg_6)
plot(Reg_6)
plot(lm.influence(Reg_6)$hat)

if (!require("olsrr")) 
{
  install.packages('olsrr');
  library(rolsrr);
}
library(olsrr)
require(olsrr)
ols_plot_dffits(Reg_6)
```

## Prueba de Hipotesis 

Pruebe la hipótesis según la cual un aumento de 1% en los gastos de A se
compensa por un aumento de 1% en los gastos de B. Utilice las pruebas lr,Wald y LM

Ho: b1-b2=0   Ha: b1-b2!=0

### Estimacion del modelo 6 restringido
```{r}

"ESTIMACIÓN DEL MODELO 6 RESTINGIDO CON LA FUNCIÓN lm()"

if (!require("restriktor")) 
{
  install.packages('restriktor');
  library(restriktor);
}
library(restriktor)

Restricciones <- '
logExpendA-logExpendB ==0;'
Reg_6R = restriktor(Reg_6, constraints = Restricciones)
summary(Reg_6R)
"Estos son los estimadores OLS del modelo 5 restringido"
(b6R=coef(Reg_6R))

sR6_mr=summary(Reg_6R)
"LA VARIANZA ESTIMADA DEL MODELO RESTRINGIDO ES"
S2_mr6=sR6_mr$s2
S2_mr6
" EL ERROR ESTÁNDAR DEL MODELO RESTRINGIDO ES"
s_mr6=(sR6_mr$s2)^0.5
s_mr6
"LA SUMA DE CUADRADOS DE LOS ERROES DEL MODELO RESTRINGIDO ES: RSS_R=S2_mr*(n-(k-1))"
RSS_R6=170*S2_mr6
RSS_R6

```

## Prueba LR Razon de Verosimilitud 
```{r}

#PRUEBA LR: RAZÓN DE VEROSIMILITUD
"PRUEBA LR: RAZÓN DE VEROSIMILITUD"
"LA SUMA DE LOS CUADRADOS DE LOS ERRORES DEL MODELO NO RESTRINGIDO ES"
"RSS_NR6"
RSS_NR6

npi=pi
n=173
k=3
"El numero e"
e=exp(1)

(LMR6=((2*npi*e/n)^(-n/2))*((RSS_R6)^(-n/2)))
"El logaritmo de la verosimilitud restringida es"
(lmr6=log(LMR6))

"LA SUMA DE LOS CUADRADOS DE LOS ERRORES DEL MODELO RESTRINGIDO ES"
"RSS_R6"
RSS_R6

"LA VEROSIMILITUD DEL MODELO NO RESTRINGIDO ES:"
(LMNR6=((2*npi*e/n)^(-n/2))*((RSS_NR6)^(-n/2)))
"El logaritmo de la verosimilitud no restringido es"
(lmnr6=log(LMNR6))

"LA ESTADÍSTICA CALCULADA DE LA ECUACIÓN 17 ES"
"LR_C_17"
(LRC_17=-2*(lmnr6-lmr6)) ##LR=2*ln(lambda)
"Valores grandes de LR nos indican que lambda tiende a ceroy por esta razón debemos rechazar Ho y decimos que logExpendA-logExpendB !==0 "

"LA ESTADÍSTICA CALCULADA DE LA ECUACIÓN 18 ES"
"LR_C_18"
(LRC_18=173*log(RSS_R6/RSS_NR6))  #LR=n*(ln(RSS_R)-ln(RSS_NR))
"EL VALOR P PARA LA PRUEBA ES"
(pv_valor=pchisq(LRC_18,df = 1, lower.tail = F))

```
Valores grandes de LR nos indican que lambda tiende a ceroy por esta razón debemos rechazar Ho y decimos que logExpendA-logExpendB !==0

##Prueba de wald

```{r}

"ESTADÍSTICA F DE WALD DE LA ECUACIÓN F (20)"
"El valor calculado de esta estadística es:"
"FC_20"
(FC_20W=n*(RSS_R6-RSS_NR6)/RSS_NR6)
"EL VALOR P PARA LA PRUEBA ES"
(pv_wald=pchisq(FC_20W,df=1,lower.tail = F))

```
## Prueba LM
Multiplicadores de Lagrange

```{r}
#options(digits=4)
"REGRESIÓN AUXILIAR PARA LA PRUEBA LM"
"Errores de estimación etr del modelo restingido vs X2t, X2t"
etr6=resid(Reg_6R)
Reg_6aux=lm(etr6~logExpendA+logExpendB+prtystrA)
(sReg_6aux_lm=summary(Reg_6aux))


"El R2_aux para la prueba LM es el R2 de la regresión auxiliar"
"El R2_aux es"
R2_aux=0.7638

R2_aux

(LM6_22=n*R2_aux)
(LM6_23=n*(RSS_R6- RSS_NR6)/(RSS_R6))
(pv_LM6=pchisq(LM6_22,df=1,lower.tail = F))
(pv_LM6_23=pchisq(LM6_23,df=1,lower.tail = F))

"Ya que P_val <<alpha concluimos que se rechaza Ho y por lo tanto logExpendA-logExpendB!==0 "
```
Ya que P_val <<alpha concluimos que se rechaza Ho y por lo tanto logExpendA-logExpendB!==0

Las tres pruebas nos hacen concluir que se debe rechazar la hipotesis nula y por lo tanto un cambio del 1% del gasto de A no es compensado por un gasto del 1% del gasto de B, ya que esto se representa como elasticidades tiene sentido hablar de porcentajes, luego logExpendA-logExpendB es diferente de cero y los cambios no son iguales por conclusion.






